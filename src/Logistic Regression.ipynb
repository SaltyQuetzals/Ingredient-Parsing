{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package treebank to /home/quontas/nltk_data...\n",
      "[nltk_data]   Package treebank is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from typing import Tuple, List\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "import random\n",
    "import nltk\n",
    "nltk.download('treebank')\n",
    "from nltk.corpus import treebank\n",
    "from collections import defaultdict\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "_uuid": "75b484df587766707357e1fd995b29a18480e4e1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<function <lambda> at 0x7f2d46f29730>,\n",
      "            {'#': 45,\n",
      "             '$': 34,\n",
      "             \"''\": 29,\n",
      "             ',': 1,\n",
      "             '-LRB-': 37,\n",
      "             '-NONE-': 16,\n",
      "             '-RRB-': 38,\n",
      "             '.': 10,\n",
      "             ':': 31,\n",
      "             'CC': 13,\n",
      "             'CD': 2,\n",
      "             'DT': 7,\n",
      "             'EX': 28,\n",
      "             'FW': 41,\n",
      "             'IN': 9,\n",
      "             'JJ': 4,\n",
      "             'JJR': 32,\n",
      "             'JJS': 25,\n",
      "             'LS': 44,\n",
      "             'MD': 5,\n",
      "             'NN': 8,\n",
      "             'NNP': 0,\n",
      "             'NNPS': 35,\n",
      "             'NNS': 3,\n",
      "             'PDT': 39,\n",
      "             'POS': 26,\n",
      "             'PRP': 19,\n",
      "             'PRP$': 24,\n",
      "             'RB': 17,\n",
      "             'RBR': 20,\n",
      "             'RBS': 40,\n",
      "             'RP': 23,\n",
      "             'SYM': 43,\n",
      "             'TO': 18,\n",
      "             'UH': 42,\n",
      "             'VB': 6,\n",
      "             'VBD': 14,\n",
      "             'VBG': 12,\n",
      "             'VBN': 15,\n",
      "             'VBP': 22,\n",
      "             'VBZ': 11,\n",
      "             'WDT': 21,\n",
      "             'WP': 30,\n",
      "             'WP$': 36,\n",
      "             'WRB': 33,\n",
      "             '``': 27})\n"
     ]
    }
   ],
   "source": [
    "pos_tags = [\n",
    "    tag for sentence in treebank.tagged_sents() \n",
    "    for _, tag in sentence\n",
    "]\n",
    "pos_tag_map = defaultdict(lambda: len(pos_tag_map))\n",
    "for tag in pos_tags:\n",
    "    pos_tag_map[tag]\n",
    "print(pos_tag_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "TAG_MAP = {\n",
    "    \"OTHER\": -1,\n",
    "    \"B-NAME\": 0,\n",
    "    \"I-NAME\": 1,\n",
    "    \"B-COMMENT\": 2,\n",
    "    \"I-COMMENT\": 3,\n",
    "    \"B-RANGE_END\": 4,\n",
    "    \"B-UNIT\": 5,\n",
    "    \"I-UNIT\": 6,\n",
    "    \"B-QTY\": 7,\n",
    "}\n",
    "\n",
    "REVERSE_MAP = {v: k for k, v in TAG_MAP.items()}\n",
    "\n",
    "\n",
    "def parse_recipe(recipe: str) -> Tuple[List[str], List[int]]:\n",
    "    \"\"\"Given a CRF-tagged recipe string, converts it into a token/tag sequence.\n",
    "\n",
    "    Args:\n",
    "        recipe: A newline-delimited CRF recipe.\n",
    "    Returns:\n",
    "        A tuple of (tokens, tags) where tokens are List[str], and tags are List[int]\n",
    "    \"\"\"\n",
    "    rows = recipe.split(\"\\n\")\n",
    "    tokens = []\n",
    "    tags = []\n",
    "    for row in rows:\n",
    "        if not row:\n",
    "            continue\n",
    "        token, _, _, _, _, tag = row.split(\"\\t\")\n",
    "        tokens.append(token)\n",
    "        tags.append(TAG_MAP[tag])\n",
    "    return tokens, tags\n",
    "\n",
    "\n",
    "def read_crf_file(filename):\n",
    "    \n",
    "    with open(filename) as f:\n",
    "        lines = f.read()\n",
    "        recipes = lines.split(\"\\n\\n\")\n",
    "\n",
    "        tokens = []\n",
    "        tags = []\n",
    "        for recipe in recipes:\n",
    "            recipe_tokens, recipe_tags = parse_recipe(recipe)\n",
    "            tokens.append(recipe_tokens)\n",
    "            tags.append(recipe_tags)\n",
    "        return tokens, tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "_uuid": "1c8f87348f706d77d700ba6d3981ccc31e84d1ed"
   },
   "outputs": [],
   "source": [
    "def get_feature(token, token_index, sent, pos_tags):\n",
    "    \"\"\"Extract features of given word(token)\"\"\"\n",
    "    token_feature = {    \n",
    "        'token'             : token,                                    # Token itself\n",
    "        'is_first'          : token_index == 0,                         # Is token at the beginning of the sentence\n",
    "        'is_last'           : token_index == len(sent)-1,               # Is token at the end of the sentence\n",
    "\n",
    "        'is_capitalized'    : token[0].upper() == token[0],             # Is first letter of token a capital letter\n",
    "        'is_all_capitalized': token.upper() == token,                   # Are all letters of token capital letters\n",
    "        'is_capitals_inside': token[1:].lower() != token[1:],           # Is there any capital letters in the token\n",
    "        'is_numeric'        : token.isdigit(),                          # Is there any digits in the token\n",
    "\n",
    "        'prefix-1'          : token[0],                                 # Token prefix containing only one letter\n",
    "        'prefix-2'          : '' if len(token) < 2  else token[:1],     # Token prefix containing two letters\n",
    "\n",
    "        'suffix-1'          : token[-1],                                # Token suffix containing only one letter\n",
    "        'suffix-2'          : '' if len(token) < 2  else token[-2:],    # Token suffix containing two letters\n",
    "\n",
    "        'prev-token'        : '' if token_index == 0     else sent[token_index - 1][0],     # Previous token in the sentence\n",
    "        '2-prev-token'      : '' if token_index <= 1     else sent[token_index - 2][0],     # Two previous token in the sentence\n",
    "\n",
    "        'next-token'        : '' if token_index == len(sent) - 1     else sent[token_index + 1][0],     # Next token in the sentence\n",
    "        '2-next-token'      : '' if token_index >= len(sent) - 2     else sent[token_index + 2][0],      # Two next token in the sentence\n",
    "        'pos-tag'           : pos_tag_map[pos_tags[token_index]]\n",
    "    }\n",
    "    return token_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_data(filename):\n",
    "    tokens, tags = read_crf_file(filename)\n",
    "    X = []\n",
    "    y = []\n",
    "    for token_list, tag_list in zip(tokens, tags):\n",
    "        sentence_pos_tags = nltk.pos_tag(token_list)\n",
    "        sentence = ' '.join(token_list)\n",
    "        \n",
    "        for i, token in enumerate(token_list):\n",
    "            X.append(get_feature(token, i, sentence, sentence_pos_tags))\n",
    "            y.append(tag_list[i])\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "26ed80a7ccc9bdf4ba2b34085e5c83297dc2c28d"
   },
   "outputs": [],
   "source": [
    "X, y = construct_data('../data/train.crftags')\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1280e11cb1f09b97ac6beee7e2ee2b42c22264bf"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/quontas/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/home/quontas/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:459: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "pipeline = make_pipeline(DictVectorizer(), LogisticRegression(class_weight='balanced'))\n",
    "\n",
    "pipeline = pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8532a492e89b8d10c28863b29389cc5d3f46902f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Accuracy: 72.84365964101912%'\n"
     ]
    }
   ],
   "source": [
    "y_predicted = pipeline.predict(X_valid)\n",
    "print(f'Accuracy: {accuracy_score(y_valid, y_predicted)*100}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8248d9deb532a8657274bc3f93c4413a75043ddd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Accuracy: 74.57286432160805%'\n"
     ]
    }
   ],
   "source": [
    "X_test, y_test = construct_data('../data/test.crftags')\n",
    "\n",
    "y_test_predictions = pipeline.predict(X_test)\n",
    "print(f'Accuracy: {accuracy_score(y_test, y_test_predictions)*100}%')\n",
    "test_tokens = [x['token'] for x in X_test]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
